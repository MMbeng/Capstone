{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205f9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\meme machine\\anaconda3\\Lib\\site-packages\\timm\\optim\\optim_factory.py:7: FutureWarning: Importing from timm.optim.optim_factory is deprecated, please import via timm.optim\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.optim\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55390b900fe407591a5f9be40abf14e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\meme machine\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\meme machine\\.cache\\huggingface\\hub\\models--timm--vit_large_patch16_224.augreg_in21k_ft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\meme machine\\AppData\\Local\\Temp\\ipykernel_1028\\2564356738.py:287: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=USE_CUDA_AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "import json, random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from timm.optim import optim_factory\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "BASE_DIR: Path = Path(r\"C:\\Users\\meme machine\\Downloads\\Capstone-20250618T001711Z-1-001\\Capstone\\project\")\n",
    "IMG_SIZE = 224\n",
    "PATCH_SIZE = 16\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 5\n",
    "NUM_WORKERS = 4\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "GRAD_CLIP_NORM = 1.0\n",
    "AUG_HFLIP = True\n",
    "GRAD_CHECKPOINT = True\n",
    "LABEL_SMOOTH = 0.05\n",
    "CLS_WEIGHT = 1.0\n",
    "REG_WEIGHT = 1.0\n",
    "IOU_WEIGHT = 0.5\n",
    "MODEL_NAME = \"vit_large_patch16_224\"\n",
    "SEED = 42\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA_AMP = (DEVICE.type == \"cuda\")\n",
    "\n",
    "RUN_DIR = Path(\"runs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.use_deterministic_algorithms(False)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "def make_patch_centers(img_size: int = 224, patch: int = 16):\n",
    "    xs, ys = np.meshgrid(\n",
    "        np.arange(patch // 2, img_size, patch),\n",
    "        np.arange(patch // 2, img_size, patch),\n",
    "    )\n",
    "    return torch.tensor(np.stack([xs, ys], -1).reshape(-1, 2), dtype=torch.float32)\n",
    "\n",
    "PATCH_CENTERS = make_patch_centers(IMG_SIZE, PATCH_SIZE)\n",
    "NUM_PATCHES = PATCH_CENTERS.shape[0]\n",
    "\n",
    "def load_lvis_mapping(base_dir: Path):\n",
    "    p_train = base_dir / \"annotations\" / \"lvis_v1_train.json\"\n",
    "    p_val = base_dir / \"annotations\" / \"lvis_v1_val.json\"\n",
    "    p = p_train if p_train.exists() else p_val\n",
    "    data = json.load(open(p, \"r\", encoding=\"utf-8\"))\n",
    "    cats = sorted(data[\"categories\"], key=lambda c: c[\"id\"])\n",
    "    id_to_idx = {c[\"id\"]: i for i, c in enumerate(cats)}\n",
    "    names = [c[\"name\"] for c in cats]\n",
    "    return id_to_idx, names\n",
    "\n",
    "CAT_ID_TO_IDX, CAT_NAMES = load_lvis_mapping(BASE_DIR)\n",
    "NUM_CLASSES = len(CAT_NAMES)\n",
    "NUM_LOGITS = NUM_CLASSES + 1\n",
    "\n",
    "class LVISDataset(Dataset):\n",
    "    def __init__(self, split: str, transform: T.Compose, id_to_idx: Dict[int, int], aug_hflip: bool = False):\n",
    "        root = BASE_DIR / f\"{split}2017\"\n",
    "        ann = BASE_DIR / \"annotations\" / f\"lvis_v1_{split}.json\"\n",
    "        self.root, self.transform, self.aug_hflip = root, transform, aug_hflip\n",
    "        with open(ann, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        self.images: Dict[int, dict] = {im[\"id\"]: im for im in data[\"images\"]}\n",
    "        self.ann_map: Dict[int, List[dict]] = {}\n",
    "        for a in data[\"annotations\"]:\n",
    "            if a.get(\"iscrowd\", 0):\n",
    "                continue\n",
    "            self.ann_map.setdefault(a[\"image_id\"], []).append(a)\n",
    "        self.ids = list(self.images.keys())\n",
    "        self.id_to_idx = id_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def _resize_boxes(self, boxes, sx, sy):\n",
    "        out = []\n",
    "        for (x, y, w, h) in boxes:\n",
    "            out.append([x * sx, y * sy, (x + w) * sx, (y + h) * sy])\n",
    "        return out\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        iid = self.ids[idx]\n",
    "        info = self.images[iid]\n",
    "        img_path = self.root / info[\"file_name\"]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        sx, sy = IMG_SIZE / info[\"width\"], IMG_SIZE / info[\"height\"]\n",
    "        anns = self.ann_map.get(iid, [])\n",
    "        boxes_coco = [ann[\"bbox\"] for ann in anns]\n",
    "        labels = [self.id_to_idx[int(ann[\"category_id\"])] for ann in anns]\n",
    "        boxes_xyxy = self._resize_boxes(boxes_coco, sx, sy)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        boxes = torch.tensor(boxes_xyxy, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        if self.aug_hflip and random.random() < 0.5:\n",
    "            img = TF.hflip(img)\n",
    "            if boxes.numel() > 0:\n",
    "                x1, y1, x2, y2 = boxes.unbind(-1)\n",
    "                new_x1 = IMG_SIZE - x2\n",
    "                new_x2 = IMG_SIZE - x1\n",
    "                boxes = torch.stack([new_x1, y1, new_x2, y2], dim=-1)\n",
    "        return img, {\"boxes\": boxes, \"labels\": labels}\n",
    "\n",
    "def collate(batch):\n",
    "    imgs, tgts = zip(*batch)\n",
    "    return list(imgs), list(tgts)\n",
    "\n",
    "class ViTDetector(nn.Module):\n",
    "    def __init__(self, num_classes: int, model_name: str = \"vit_base_patch16_224\", grad_checkpoint: bool = False):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0, drop_path_rate=0.1)\n",
    "        if grad_checkpoint and hasattr(self.backbone, \"set_grad_checkpointing\"):\n",
    "            self.backbone.set_grad_checkpointing()\n",
    "        d = self.backbone.num_features\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d),\n",
    "            nn.Linear(d, d),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(d, 4 + num_classes + 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        feats = self.backbone.forward_features(x)[:, 1:]\n",
    "        out = self.head(feats)\n",
    "        return out\n",
    "\n",
    "def iou_aligned(b1: torch.Tensor, b2: torch.Tensor):\n",
    "    x1 = torch.max(b1[:, 0], b2[:, 0])\n",
    "    y1 = torch.max(b1[:, 1], b2[:, 1])\n",
    "    x2 = torch.min(b1[:, 2], b2[:, 2])\n",
    "    y2 = torch.min(b1[:, 3], b2[:, 3])\n",
    "    iw = (x2 - x1).clamp(min=0)\n",
    "    ih = (y2 - y1).clamp(min=0)\n",
    "    inter = iw * ih\n",
    "    a1 = (b1[:, 2] - b1[:, 0]).clamp(min=0) * (b1[:, 3] - b1[:, 1]).clamp(min=0)\n",
    "    a2 = (b2[:, 2] - b2[:, 0]).clamp(min=0) * (b2[:, 3] - b2[:, 1]).clamp(min=0)\n",
    "    union = a1 + a2 - inter + 1e-6\n",
    "    return inter / union\n",
    "\n",
    "class PatchCriterion(nn.Module):\n",
    "    def __init__(self, cls_w=1.0, reg_w=1.0, iou_w=0.5, label_smooth=0.05):\n",
    "        super().__init__()\n",
    "        self.cls_loss = nn.CrossEntropyLoss(label_smoothing=label_smooth)\n",
    "        self.reg_loss = nn.SmoothL1Loss()\n",
    "        self.cls_w = cls_w\n",
    "        self.reg_w = reg_w\n",
    "        self.iou_w = iou_w\n",
    "\n",
    "    def assign(self, boxes: torch.Tensor, labels: torch.Tensor):\n",
    "        P = NUM_PATCHES\n",
    "        device = boxes.device\n",
    "        cls_t = torch.zeros(P, dtype=torch.long, device=device)\n",
    "        reg_t = torch.zeros(P, 4, dtype=torch.float32, device=device)\n",
    "        mask = torch.zeros(P, dtype=torch.bool, device=device)\n",
    "        if boxes.numel() == 0:\n",
    "            return cls_t, reg_t, mask\n",
    "        centers = PATCH_CENTERS.to(device)\n",
    "        cx, cy = centers[:, 0], centers[:, 1]\n",
    "        for box, lbl in zip(boxes, labels):\n",
    "            x1, y1, x2, y2 = box\n",
    "            inside = (cx >= x1) & (cx <= x2) & (cy >= y1) & (cy <= y2)\n",
    "            if inside.any():\n",
    "                cls_t[inside] = int(lbl) + 1\n",
    "                reg_t[inside] = box\n",
    "                mask[inside] = True\n",
    "        return cls_t, reg_t, mask\n",
    "\n",
    "    def forward(self, pred: torch.Tensor, targets: List[dict]):\n",
    "        B, P, _ = pred.shape\n",
    "        reg_p, cls_p = pred[..., :4], pred[..., 4:]\n",
    "        cls_t_all = torch.zeros(B, P, dtype=torch.long, device=pred.device)\n",
    "        reg_t_all = torch.zeros(B, P, 4, dtype=torch.float32, device=pred.device)\n",
    "        mask_all = torch.zeros(B, P, dtype=torch.bool, device=pred.device)\n",
    "        for i, tgt in enumerate(targets):\n",
    "            c, r, m = self.assign(tgt[\"boxes\"].to(pred.device), tgt[\"labels\"].to(pred.device))\n",
    "            cls_t_all[i], reg_t_all[i], mask_all[i] = c, r, m\n",
    "        cls_loss = self.cls_loss(cls_p.permute(0, 2, 1), cls_t_all)\n",
    "        if mask_all.any():\n",
    "            rp = reg_p[mask_all]\n",
    "            rt = reg_t_all[mask_all]\n",
    "            reg_loss = self.reg_loss(rp, rt)\n",
    "            iou = iou_aligned(rp, rt)\n",
    "            iou_loss = (1.0 - iou).mean()\n",
    "        else:\n",
    "            reg_loss = torch.tensor(0.0, device=pred.device)\n",
    "            iou_loss = torch.tensor(0.0, device=pred.device)\n",
    "        return self.cls_w * cls_loss + self.reg_w * reg_loss + self.iou_w * iou_loss\n",
    "\n",
    "criterion = PatchCriterion(CLS_WEIGHT, REG_WEIGHT, IOU_WEIGHT, LABEL_SMOOTH).to(DEVICE)\n",
    "\n",
    "tf_train = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ColorJitter(0.1, 0.1, 0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5] * 3, [0.5] * 3),\n",
    "])\n",
    "\n",
    "tf_val = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5] * 3, [0.5] * 3),\n",
    "])\n",
    "\n",
    "def make_loaders(workers: int = 4):\n",
    "    train_ds = LVISDataset(\"train\", tf_train, CAT_ID_TO_IDX, aug_hflip=AUG_HFLIP)\n",
    "    val_ds = LVISDataset(\"val\", tf_val, CAT_ID_TO_IDX, aug_hflip=False)\n",
    "    train_ld = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=workers, collate_fn=collate, pin_memory=True, persistent_workers=(workers > 0))\n",
    "    val_ld = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=workers, collate_fn=collate, pin_memory=True, persistent_workers=(workers > 0))\n",
    "    return train_ld, val_ld\n",
    "\n",
    "def build_optimizer(model):\n",
    "    param_groups = optim_factory.param_groups_weight_decay(model, weight_decay=WEIGHT_DECAY)\n",
    "    opt = torch.optim.AdamW(param_groups, lr=LR)\n",
    "    return opt\n",
    "\n",
    "def build_scheduler(opt, train_steps_total):\n",
    "    warmup_steps = max(100, int(0.05 * train_steps_total))\n",
    "    sched1 = torch.optim.lr_scheduler.LinearLR(opt, start_factor=0.1, total_iters=warmup_steps)\n",
    "    sched2 = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max(1, train_steps_total - warmup_steps))\n",
    "    sched = torch.optim.lr_scheduler.SequentialLR(opt, schedulers=[sched1, sched2], milestones=[warmup_steps])\n",
    "    return sched\n",
    "\n",
    "def train_epoch(model, dl, opt, scaler, scheduler, epoch: int):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "    for imgs, tgts in dl:\n",
    "        imgs = torch.stack(imgs).to(DEVICE, non_blocking=True)\n",
    "        with autocast(device_type=\"cuda\", enabled=USE_CUDA_AMP):\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, tgts)\n",
    "        if USE_CUDA_AMP:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
    "            opt.step()\n",
    "        scheduler.step()\n",
    "        total += float(loss.item())\n",
    "        n += 1\n",
    "    return total / max(n, 1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, dl):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "    for imgs, tgts in dl:\n",
    "        imgs = torch.stack(imgs).to(DEVICE, non_blocking=True)\n",
    "        with autocast(device_type=\"cuda\", enabled=USE_CUDA_AMP):\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, tgts)\n",
    "        total += float(loss.item())\n",
    "        n += 1\n",
    "    return total / max(n, 1)\n",
    "print(f\"Epoch\")\n",
    "\n",
    "def main():\n",
    "    train_ld, val_ld = make_loaders(workers=NUM_WORKERS)\n",
    "    model = ViTDetector(NUM_CLASSES, model_name=MODEL_NAME, grad_checkpoint=GRAD_CHECKPOINT).to(DEVICE)\n",
    "    opt = build_optimizer(model)\n",
    "    scaler = GradScaler(enabled=USE_CUDA_AMP)\n",
    "    scheduler = build_scheduler(opt, EPOCHS * len(train_ld))\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "        tr_loss = train_epoch(model, train_ld, opt, scaler, scheduler, epoch)\n",
    "        val_loss = validate(model, val_ld)\n",
    "        print(f\"Epoch {epoch}: train {tr_loss:.4f} | val {val_loss:.4f}\")\n",
    "        torch.save(model.state_dict(), RUN_DIR / f\"epoch{epoch:02d}.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(\"CUDA out of memory. Reduce BATCH_SIZE or IMG_SIZE, or keep GRAD_CHECKPOINT=True.\")\n",
    "            raise\n",
    "        else:\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84077ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_infer = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "\n",
    "def latest_checkpoint(runs_dir=Path(\"runs\")):\n",
    "    pths = []\n",
    "    if runs_dir.exists():\n",
    "        for root, _, files in os.walk(runs_dir):\n",
    "            for f in files:\n",
    "                if f.endswith(\".pth\"):\n",
    "                    pths.append(Path(root) / f)\n",
    "    if not pths:\n",
    "        raise FileNotFoundError(\"no checkpoints found in runs/\")\n",
    "    pths.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return pths[0]\n",
    "\n",
    "def load_id_to_class(base_dir: Path):\n",
    "    ann_val = base_dir / \"annotations\" / \"lvis_v1_val.json\"\n",
    "    ann_train = base_dir / \"annotations\" / \"lvis_v1_train.json\"\n",
    "    path = ann_val if ann_val.exists() else ann_train\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    data = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
    "    return {c[\"id\"]: c[\"name\"] for c in data[\"categories\"]}\n",
    "\n",
    "class ViTDetector(torch.nn.Module):\n",
    "    def __init__(self, num_classes: int, grad_checkpoint: bool = False):\n",
    "        super().__init__()\n",
    "        import timm\n",
    "        self.backbone = timm.create_model(\"vit_base_patch16_224\", pretrained=False, num_classes=0)\n",
    "        if grad_checkpoint and hasattr(self.backbone, \"set_grad_checkpointing\"):\n",
    "            self.backbone.set_grad_checkpointing()\n",
    "        d = self.backbone.num_features\n",
    "        self.head = torch.nn.Linear(d, 4 + num_classes + 1)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        feats = self.backbone.forward_features(x)[:, 1:]\n",
    "        out = self.head(feats)\n",
    "        return out\n",
    "\n",
    "def predict_image(img_path: Path, model, id_to_class, score_thresh=0.5, iou_thresh=0.5, max_dets=100):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = tf_infer(img).unsqueeze(0).to(DEVICE)\n",
    "    with torch.inference_mode():\n",
    "        with torch.cuda.amp.autocast(enabled=USE_CUDA_AMP):\n",
    "            out = model(x)\n",
    "            reg = out[..., :4][0]\n",
    "            cls = out[..., 4:][0]\n",
    "            prob = F.softmax(cls, dim=-1)\n",
    "            scores, labels = prob.max(dim=-1)\n",
    "            keep = (labels != 0) & (scores >= score_thresh)\n",
    "            if keep.sum() == 0:\n",
    "                return img\n",
    "            boxes = reg[keep]\n",
    "            scores = scores[keep]\n",
    "            labels = labels[keep] - 1\n",
    "            final_boxes = []\n",
    "            final_scores = []\n",
    "            final_labels = []\n",
    "            for c in labels.unique():\n",
    "                idx = torch.nonzero(labels == c, as_tuple=False).squeeze(1)\n",
    "                b = boxes[idx]\n",
    "                s = scores[idx]\n",
    "                keep_idx = nms(b, s, iou_thresh)\n",
    "                keep_idx = keep_idx[:max_dets]\n",
    "                final_boxes.append(b[keep_idx])\n",
    "                final_scores.append(s[keep_idx])\n",
    "                final_labels.append(labels[idx][keep_idx])\n",
    "            boxes = torch.cat(final_boxes, dim=0)\n",
    "            scores = torch.cat(final_scores, dim=0)\n",
    "            labels = torch.cat(final_labels, dim=0)\n",
    "    draw = ImageDraw.Draw(img.resize((IMG_SIZE, IMG_SIZE)))\n",
    "    for b, s, l in zip(boxes, scores, labels):\n",
    "        x1, y1, x2, y2 = [v.item() for v in b.clamp(0, IMG_SIZE)]\n",
    "        name = id_to_class.get(int(l.item()), f\"id_{int(l.item())}\")\n",
    "        draw.rectangle([(x1, y1), (x2, y2)], outline=(255, 0, 0), width=2)\n",
    "        draw.text((x1 + 2, y1 + 2), f\"{name} {s.item():.2f}\")\n",
    "    return img\n",
    "\n",
    "def load_model(ckpt_path: Path):\n",
    "    model = ViTDetector(NUM_CLASSES, grad_checkpoint=False).to(DEVICE)\n",
    "    sd = torch.load(ckpt_path, map_location=DEVICE)\n",
    "    model.load_state_dict(sd, strict=True)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "ckpt = latest_checkpoint()\n",
    "model = load_model(ckpt)\n",
    "id_to_class = load_id_to_class(BASE_DIR)\n",
    "VAL_DIR = BASE_DIR / \"val2017\"\n",
    "paths = sorted([p for p in VAL_DIR.glob(\"*.jpg\")])[:8] if VAL_DIR.exists() else []\n",
    "OUT_DIR = Path(\"preds\")\n",
    "OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "results = []\n",
    "for p in paths:\n",
    "    img_pred = predict_image(p, model, id_to_class, score_thresh=0.5, iou_thresh=0.5)\n",
    "    save_path = OUT_DIR / f\"{p.stem}_pred.jpg\"\n",
    "    img_pred.save(save_path)\n",
    "    results.append(save_path)\n",
    "\n",
    "for p in results:\n",
    "    display(Image.open(p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
