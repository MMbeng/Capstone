{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "import timm\n",
    "import numpy as np\n",
    "BASE_DIR = r\"C:\\Users\\meme machine\\Downloads\\Capstone-20250618T001711Z-1-001\\Capstone\\project\"\n",
    "IMG_SIZE = 224\n",
    "PATCH_SIZE = 16\n",
    "NUM_CLASSES = 1204\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ColorJitter(0.1, 0.1, 0.1),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "class ManualLVISDataset(Dataset):\n",
    "    def __init__(self, image_root, ann_file, transforms=None):\n",
    "        self.image_root = image_root\n",
    "        self.transforms = transforms\n",
    "        with open(ann_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.images = data['images']\n",
    "        self.annotations = data['annotations']\n",
    "        self.image_to_anns = {}\n",
    "        for ann in self.annotations:\n",
    "            self.image_to_anns.setdefault(ann['image_id'], []).append(ann)\n",
    "        self.id_to_image = {img['id']: img for img in self.images}\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_info = self.images[index]\n",
    "        img_id = img_info['id']\n",
    "        file_name = img_info.get(\"file_name\", f\"{img_id:012d}.jpg\")\n",
    "        img_path = os.path.join(self.image_root, file_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        anns = self.image_to_anns.get(img_id, [])\n",
    "        return image, anns\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "def get_dataset(split, transform):\n",
    "    img_dir = os.path.join(BASE_DIR, f\"{split}2017\")\n",
    "    ann_path = os.path.join(BASE_DIR, \"annotations\", f\"lvis_v1_{split}.json\")\n",
    "    return ManualLVISDataset(img_dir, ann_path, transform)\n",
    "\n",
    "train_dataset = get_dataset(\"train\", transform)\n",
    "val_dataset = get_dataset(\"val\", transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "class ViTDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.vit = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=0)\n",
    "        self.hidden_dim = self.vit.embed_dim\n",
    "        self.head = nn.Linear(self.hidden_dim, 4 + num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        feats = self.vit.forward_features(x)[:, 1:] \n",
    "        out = self.head(feats)  \n",
    "        return out\n",
    "\n",
    "model = ViTDetectionModel(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch_center_coords(img_size=224, patch_size=16):\n",
    "    coords = []\n",
    "    for y in range(0, img_size, patch_size):\n",
    "        for x in range(0, img_size, patch_size):\n",
    "            cx = x + patch_size // 2\n",
    "            cy = y + patch_size // 2\n",
    "            coords.append((cx, cy))\n",
    "    return coords  \n",
    "\n",
    "PATCH_CENTERS = get_patch_center_coords(IMG_SIZE, PATCH_SIZE)\n",
    "\n",
    "def assign_gt_to_patches(boxes):\n",
    "    patch_targets = [-1] * len(PATCH_CENTERS)\n",
    "    bbox_targets = [None] * len(PATCH_CENTERS)\n",
    "\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "        dists = [(px - cx)**2 + (py - cy)**2 for px, py in PATCH_CENTERS]\n",
    "        min_idx = np.argmin(dists)\n",
    "        patch_targets[min_idx] = box\n",
    "    return patch_targets\n",
    "\n",
    "def detection_loss(preds, targets):\n",
    "    cls_loss_fn = nn.CrossEntropyLoss()\n",
    "    reg_loss_fn = nn.SmoothL1Loss()\n",
    "    total_cls_loss, total_reg_loss = 0.0, 0.0\n",
    "    for i in range(len(preds)):\n",
    "        pred = preds[i] \n",
    "        gt = targets[i]\n",
    "        boxes = gt['boxes'].to(DEVICE)\n",
    "        labels = gt['labels'].to(DEVICE)\n",
    "\n",
    "        if boxes.numel() == 0 or labels.numel() == 0:\n",
    "            continue  \n",
    "\n",
    "        assignments = assign_gt_to_patches(boxes)\n",
    "        for patch_idx, box in enumerate(assignments):\n",
    "            if box is None:\n",
    "                continue\n",
    "\n",
    "            label = torch.tensor(labels[0], dtype=torch.long, device=DEVICE)\n",
    "            box_tensor = box.to(DEVICE) if isinstance(box, torch.Tensor) else torch.tensor(box, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "            pred_box = pred[patch_idx, :4]\n",
    "            pred_cls = pred[patch_idx, 4:]\n",
    "\n",
    "            total_reg_loss += reg_loss_fn(pred_box, box_tensor)\n",
    "            total_cls_loss += cls_loss_fn(pred_cls.unsqueeze(0), label.unsqueeze(0))\n",
    "\n",
    "\n",
    "\n",
    "    return total_cls_loss + total_reg_loss\n",
    "\n",
    "def preprocess_targets(targets):\n",
    "    results = []\n",
    "    for ann_list in targets:\n",
    "        boxes, labels = [], []\n",
    "        for ann in ann_list:\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            boxes.append(torch.tensor([x, y, x + w, y + h]))\n",
    "            labels.append(ann[\"category_id\"])\n",
    "        results.append({\n",
    "            \"boxes\": torch.stack(boxes) if boxes else torch.zeros((0, 4)),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,), dtype=torch.int64)\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for images, targets in dataloader:\n",
    "        images = torch.stack(images).to(DEVICE)\n",
    "        targets = preprocess_targets(targets)\n",
    "        preds = model(images)\n",
    "        loss = detection_loss(preds, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    for images, targets in dataloader:\n",
    "        images = torch.stack(images).to(DEVICE)\n",
    "        preds = model(images)\n",
    "        print(\"Validation output shape:\", preds.shape)\n",
    "        break\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "for epoch in range(5):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer)\n",
    "    print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f}\")\n",
    "    evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(BASE_DIR, \"annotations\", \"lvis_v1_train.json\"), \"r\") as f:\n",
    "    lvis_data = json.load(f)\n",
    "\n",
    "id_to_class = {cat[\"id\"]: cat[\"name\"] for cat in lvis_data[\"categories\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and ready.\n"
     ]
    }
   ],
   "source": [
    "# model_save_path = os.path.join(BASE_DIR, \"vit_llvis_model.pth\")\n",
    "# torch.save(model.state_dict(), model_save_path)\n",
    "# print(f\"Model saved to: {model_save_path}\")\n",
    "model = ViTDetectionModel(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "model.load_state_dict(torch.load(os.path.join(BASE_DIR, \"vit_llvis_model.pth\"), map_location=DEVICE))\n",
    "model.eval()\n",
    "print(\"Model loaded and ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
